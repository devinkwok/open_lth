# Copyright (c) Facebook, Inc. and its affiliates.

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import numpy as np
import os
from PIL import Image
import sys
import torchvision

from datasets import base
from platforms.platform import get_platform


class CIFAR10(torchvision.datasets.CIFAR10):
    """A subclass to suppress an annoying print statement in the torchvision CIFAR-10 library.

    Not strictly necessary - you can just use `torchvision.datasets.CIFAR10 if the print
    message doesn't bother you.
    """

    def download(self):
        if get_platform().is_primary_process:
            with get_platform().open(os.devnull, 'w') as fp:
                sys.stdout = fp
                super(CIFAR10, self).download()
                sys.stdout = sys.__stdout__
        get_platform().barrier()


class Dataset(base.ImageDataset, base.NdarrayDataset):
    """The CIFAR-10 dataset with a fixed random permutation of the input pixels.
    This breaks up features typically recognized by ConvNets.
    """
    PIXEL_PERMUTATION = [29,132,278,149,102,688,22,308,972,183,1018,790,985,635,482,469,431,55,933,531,374,454,77,658,70,626,505,8,664,211,1003,290,921,223,317,597,973,304,4,7,342,136,761,418,588,630,1007,740,450,173,823,477,852,425,321,824,857,868,507,463,984,840,867,398,638,816,424,932,547,126,882,150,12,193,818,479,1020,899,377,995,640,963,743,958,69,789,185,261,627,885,340,67,1000,878,349,988,569,508,476,649,118,137,138,667,554,486,309,737,659,0,764,855,725,914,767,509,561,343,114,541,946,976,222,417,48,865,610,897,1005,801,707,624,366,439,465,774,730,542,910,975,786,797,285,735,255,601,419,580,523,491,558,156,999,589,380,397,254,951,500,905,229,848,201,808,20,265,895,680,189,912,571,144,474,521,40,282,982,238,354,775,513,329,388,235,738,434,257,598,154,665,653,68,604,344,498,328,683,1012,673,641,674,318,918,901,168,13,639,333,493,965,496,214,697,279,164,448,1022,581,534,350,217,268,574,186,758,716,827,749,347,161,57,184,546,112,652,524,684,259,621,536,358,263,602,837,744,452,310,262,30,43,244,956,678,810,252,713,795,501,690,457,736,382,957,582,88,420,341,811,705,470,438,711,330,691,1004,888,723,528,315,720,835,364,100,994,376,440,518,384,246,172,17,289,412,666,432,198,750,927,681,782,63,587,120,85,847,296,693,107,10,1014,1021,204,27,175,556,813,766,829,894,473,760,359,728,221,530,71,715,293,892,785,807,979,599,516,248,662,938,625,460,677,881,701,19,922,805,817,426,947,79,974,517,97,199,644,166,828,18,504,127,152,215,700,874,853,1017,87,987,273,458,971,564,675,455,373,213,497,208,31,834,969,306,793,108,660,591,923,409,960,572,1016,849,313,648,792,617,770,539,361,461,209,103,288,903,928,400,80,240,562,822,919,453,351,802,72,429,225,966,645,35,623,64,269,619,911,396,187,116,993,353,92,563,404,540,489,908,151,948,884,830,937,549,989,253,566,91,575,931,224,362,312,687,576,307,93,747,192,669,160,122,94,357,941,26,403,21,863,325,732,82,104,590,83,84,915,864,196,862,370,844,791,578,442,459,219,515,76,45,585,437,668,239,839,787,600,134,548,174,338,512,490,169,866,821,101,121,495,596,314,955,181,783,242,842,73,356,670,607,615,980,488,727,393,39,249,89,869,950,410,423,773,256,230,605,959,708,5,274,287,416,324,636,415,475,514,568,559,391,543,560,502,216,527,171,337,567,124,51,745,642,413,876,267,86,1,930,990,188,944,372,369,851,34,954,276,733,281,179,712,62,794,772,447,483,833,435,940,593,363,111,661,352,757,756,220,843,682,695,777,206,106,99,212,334,929,633,877,519,812,280,880,520,913,200,445,584,836,378,275,50,247,36,320,176,769,970,335,953,781,920,503,522,861,311,679,148,441,893,632,968,1013,234,113,295,227,110,815,462,480,59,190,25,139,241,565,856,494,197,586,798,537,820,671,945,731,981,205,689,294,345,709,360,327,153,826,133,117,436,646,155,191,146,348,832,656,260,784,762,803,529,23,1015,552,949,298,983,487,379,427,33,752,1008,525,699,925,456,105,24,618,872,628,75,544,612,654,637,942,594,236,316,433,37,902,411,336,788,286,860,696,32,95,368,651,896,854,510,401,871,555,814,292,722,875,511,123,606,78,924,125,466,96,218,825,672,271,917,6,904,729,916,754,46,319,159,706,323,692,130,704,226,444,14,41,533,841,776,162,464,753,28,726,203,158,402,414,141,228,886,573,451,831,170,996,873,115,165,634,383,734,768,557,140,251,11,481,446,551,264,771,1011,109,81,577,302,870,647,799,719,421,545,935,991,592,272,778,232,428,962,655,936,998,622,389,65,526,42,145,299,195,449,741,98,535,177,845,967,237,16,467,742,889,850,331,694,718,394,180,964,1006,131,406,609,2,796,390,1002,492,375,570,49,478,721,538,595,900,167,806,780,611,961,748,355,613,74,977,553,60,1019,52,365,386,717,499,532,207,616,746,800,58,392,270,608,66,698,119,859,819,128,250,952,763,405,663,485,56,15,385,297,90,620,579,724,47,978,583,1001,9,759,38,676,202,53,471,408,194,305,182,399,506,231,986,381,858,277,178,614,703,371,44,906,61,472,422,407,163,1023,907,550,891,686,301,142,484,300,685,751,395,1009,346,3,147,710,367,387,883,838,322,443,643,755,258,284,702,714,245,909,992,157,303,779,283,339,631,939,129,650,332,887,657,934,233,210,54,765,809,266,326,143,629,135,846,430,890,603,739,468,1010,926,898,243,879,997,804,291,943]

    @staticmethod
    def num_train_examples(): return 50000

    @staticmethod
    def num_test_examples(): return 10000

    @staticmethod
    def num_classes(): return 10

    @staticmethod
    def get_data(train):
        dataset = CIFAR10(train=train, root=os.path.join(
            get_platform().dataset_root, 'cifar10'), download=get_platform().download_data)
        # permute pixels
        data = dataset.data.reshape(-1, 32*32, 3)
        permutation = np.array(Dataset.PIXEL_PERMUTATION).reshape(1, 32*32, 1)
        data = np.take_along_axis(data, permutation, axis=1).reshape(-1, 32, 32, 3)
        return data, np.array(dataset.targets)

    @staticmethod
    def get_train_set(use_augmentation, train_split=None):
        augment = [torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.RandomCrop(32, 4)]
        data, targets = Dataset.get_data_split(True, train_split)
        return Dataset(data, targets, augment if use_augmentation else [])

    @staticmethod
    def get_test_set(test_split=None):
        data, targets = Dataset.get_data_split(False, test_split)
        return Dataset(data, targets)

    def __init__(self,  examples, labels, image_transforms=None):
        super(Dataset, self).__init__(examples, labels, image_transforms or [],
                                      [torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

    def example_to_image(self, example):
        return Image.fromarray(example)


DataLoader = base.DataLoader
